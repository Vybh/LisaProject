# Project Setup and Execution Guide

## Quick Start (5 minutes)

### 1. Environment Setup

```bash
cd /Users/vybhavreddy/Desktop/tinylisaproject

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Dataset

Ensure your LISATS dataset is organized as:
```
/Users/vybhavreddy/Desktop/LISATS/
â”œâ”€â”€ Stop/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â”œâ”€â”€ image2.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Yield/
â”‚   â”œâ”€â”€ image1.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ [other_classes]/
```

### 3. Run the Complete Pipeline

**Option A: Use main training script**
```bash
cd src
python train.py --data_path /Users/vybhavreddy/Desktop/LISATS
```

**Option B: Use Jupyter notebook** (Interactive)
```bash
jupyter notebook notebooks/EDA.ipynb
```

---

## Project Structure Overview

```
tinylisaproject/
â”‚
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ datasets.py              # TrafficSignDataset class
â”‚   â”‚   â”œâ”€â”€ load_data()          # Load images from directories
â”‚   â”‚   â”œâ”€â”€ stratified_split()   # 70/15/15 split
â”‚   â”‚   â””â”€â”€ get_class_weights()  # Handle imbalance
â”‚   â”‚
â”‚   â”œâ”€â”€ features.py              # FeatureExtractor class
â”‚   â”‚   â”œâ”€â”€ extract_hog()        # HOG features (1764 dims)
â”‚   â”‚   â”œâ”€â”€ extract_color_histogram()  # ColorHist (96 dims)
â”‚   â”‚   â”œâ”€â”€ fit_bovw()           # Fit BoVW clustering
â”‚   â”‚   â”œâ”€â”€ extract_bovw()       # BoVW features (100 dims)
â”‚   â”‚   â”œâ”€â”€ extract_all_features()     # Concatenate all
â”‚   â”‚   â”œâ”€â”€ fit_scaler()         # Fit StandardScaler
â”‚   â”‚   â””â”€â”€ scale_features()     # Normalize
â”‚   â”‚
â”‚   â”œâ”€â”€ models.py                # ModelTrainer class
â”‚   â”‚   â”œâ”€â”€ get_models()         # Initialize 4 algorithms
â”‚   â”‚   â”œâ”€â”€ cross_validate_models()    # 5-fold CV
â”‚   â”‚   â”œâ”€â”€ train_final_models() # Train on full train set
â”‚   â”‚   â”œâ”€â”€ evaluate_models()    # Test set evaluation
â”‚   â”‚   â””â”€â”€ get_feature_importance()   # Feature analysis
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluate.py              # Evaluator class
â”‚   â”‚   â”œâ”€â”€ plot_confusion_matrix()
â”‚   â”‚   â”œâ”€â”€ plot_roc_curves()
â”‚   â”‚   â”œâ”€â”€ plot_precision_recall()
â”‚   â”‚   â”œâ”€â”€ plot_calibration_curve()
â”‚   â”‚   â”œâ”€â”€ plot_model_comparison()
â”‚   â”‚   â””â”€â”€ plot_learning_curves()
â”‚   â”‚
â”‚   â””â”€â”€ train.py                 # Main pipeline orchestrator
â”‚       â”œâ”€â”€ Load config
â”‚       â”œâ”€â”€ Load & split data
â”‚       â”œâ”€â”€ Extract features
â”‚       â”œâ”€â”€ 5-fold CV
â”‚       â”œâ”€â”€ Train final models
â”‚       â”œâ”€â”€ Evaluate
â”‚       â””â”€â”€ Save results
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ traffic_signs.yaml       # Configurable hyperparameters
â”‚       â”œâ”€â”€ Dataset paths
â”‚       â”œâ”€â”€ Feature options (HOG, ColorHist, BoVW)
â”‚       â”œâ”€â”€ Model hyperparameters
â”‚       â””â”€â”€ Evaluation settings
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ EDA.ipynb               # Interactive Jupyter notebook
â”‚       â”œâ”€â”€ Section 1: Data exploration
â”‚       â”œâ”€â”€ Section 2: Stratified splitting
â”‚       â”œâ”€â”€ Section 3: Feature extraction
â”‚       â”œâ”€â”€ Section 4: Cross-validation
â”‚       â”œâ”€â”€ Section 5: Test evaluation
â”‚       â”œâ”€â”€ Section 6: Learning curves & feature importance
â”‚       â”œâ”€â”€ Section 7: Robustness analysis
â”‚       â””â”€â”€ Section 8: Error analysis
â”‚
â”œâ”€â”€ models/                       # Trained model artifacts
â”‚   â”œâ”€â”€ SVM.pkl
â”‚   â”œâ”€â”€ RandomForest.pkl
â”‚   â”œâ”€â”€ kNN.pkl
â”‚   â””â”€â”€ XGBoost.pkl
â”‚
â”œâ”€â”€ results/                      # Evaluation artifacts
â”‚   â”œâ”€â”€ cv_results.csv           # Cross-validation scores
â”‚   â”œâ”€â”€ test_results.csv         # Test metrics
â”‚   â”œâ”€â”€ confusion_matrix_*.png    # 4 confusion matrices
â”‚   â”œâ”€â”€ roc_curves_*.png          # ROC curves (one-vs-rest)
â”‚   â”œâ”€â”€ pr_curves_*.png           # Precision-Recall curves
â”‚   â”œâ”€â”€ calibration_*.png         # Calibration plots
â”‚   â”œâ”€â”€ learning_curve_*.png      # Learning curves
â”‚   â”œâ”€â”€ model_comparison_*.png    # Metric comparison charts
â”‚   â”œâ”€â”€ summary.json              # Final results summary
â”‚   â””â”€â”€ [other visualizations]
â”‚
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ REPORT_TEMPLATE.md           # Report structure (6-8 pages)
â”œâ”€â”€ PRESENTATION_OUTLINE.md      # Slides outline (10-12 slides)
â”œâ”€â”€ setup.sh                     # Automated setup script
â””â”€â”€ SETUP_GUIDE.md              # This file
```

---

## Detailed Module Usage

### 1. datasets.py - Data Loading & Splitting

```python
from src.datasets import TrafficSignDataset

# Initialize
dataset = TrafficSignDataset(
    data_path='/Users/vybhavreddy/Desktop/LISATS',
    random_state=42
)

# Load all images
X, y = dataset.load_data()

# Stratified 70/15/15 split
X_train, X_val, X_test, y_train, y_val, y_test = dataset.stratified_split()

# Get class weights for imbalance handling
class_weights = dataset.get_class_weights(y_train)
```

### 2. features.py - Feature Extraction

```python
from src.features import FeatureExtractor

# Initialize
fe = FeatureExtractor(n_clusters=100, scaler_type='standard')

# Fit BoVW on training set
fe.fit_bovw(X_train)

# Extract all features
X_train_features = fe.extract_all_features(X_train)
X_test_features = fe.extract_all_features(X_test)

# Scale features
X_train_scaled = fe.fit_transform(X_train_features)
X_test_scaled = fe.scale_features(X_test_features)
```

**Feature Dimensions**:
- HOG: 1764 features
- ColorHist: 96 features (32 bins Ã— 3 channels)
- BoVW: 100 features (100 clusters)
- **Total**: 1960 features

### 3. models.py - Model Training

```python
from src.models import ModelTrainer

# Initialize trainer
trainer = ModelTrainer(random_state=42, n_splits=5)

# Cross-validation
cv_results = trainer.cross_validate_models(
    X_train_scaled, y_train, 
    class_weights=class_weights
)

# Train final models
trainer.train_final_models(
    X_train_scaled, y_train,
    class_weights=class_weights
)

# Evaluate on test set
test_results = trainer.evaluate_models(X_test_scaled, y_test)
```

**Models Compared**:
| Model | Hyperparameters |
|-------|-----------------|
| SVM | kernel=rbf, C=1.0, gamma=scale |
| Random Forest | n_estimators=100, max_depth=15 |
| k-NN | n_neighbors=5, weights=distance |
| XGBoost | n_estimators=100, max_depth=6, lr=0.1 |

### 4. evaluate.py - Visualization & Analysis

```python
from src.evaluate import Evaluator

evaluator = Evaluator(output_dir='results')

# Generate visualizations
evaluator.plot_confusion_matrix(y_test, y_pred, class_names, 'SVM')
evaluator.plot_roc_curves(y_test, y_proba, class_names, 'SVM')
evaluator.plot_precision_recall(y_test, y_proba, class_names, 'SVM')
evaluator.plot_calibration_curve(y_test, y_proba_max, 'SVM')
evaluator.plot_model_comparison(test_results, 'Accuracy')
```

---

## Configuration File (configs/traffic_signs.yaml)

Customize the pipeline by editing:

```yaml
# Dataset
dataset:
  path: "/Users/vybhavreddy/Desktop/LISATS"
  train_size: 0.7

# Features
features:
  bovw:
    n_clusters: 100

# Training
training:
  n_splits: 5
  models:
    random_forest:
      n_estimators: 100
      max_depth: 15
```

---

## Typical Execution Timeline

| Step | Duration | Notes |
|------|----------|-------|
| Data Loading | 1-2 min | Depends on dataset size |
| BoVW Fitting | 2-3 min | SIFT extraction & clustering |
| Feature Extraction | 3-5 min | All 3 feature types on all data |
| 5-Fold CV | 2-3 min | Training 4 models Ã— 5 folds |
| Final Training | 1 min | Train on full train set |
| Evaluation | <1 min | Test set predictions |
| Visualization | 1-2 min | Plot generation |
| **Total** | **~12-17 min** | For ~1000 images |

---

## Expected Output Files

### In `results/` directory:
- `cv_results.csv` - CV metrics for 4 models
- `test_results.csv` - Test metrics (Acc, Prec, Recall, F1)
- `confusion_matrix_SVM.png` - 4 confusion matrices
- `roc_curves_SVM.png` - ROC curves (multi-class one-vs-rest)
- `pr_curves_SVM.png` - Precision-Recall curves
- `calibration_SVM.png` - Calibration plots
- `model_comparison_Accuracy.png` - Bar chart comparison
- `summary.json` - Final results dictionary

### In `models/` directory:
- `SVM.pkl` - Trained SVM model
- `RandomForest.pkl` - Trained RF model
- `kNN.pkl` - Trained k-NN model
- `XGBoost.pkl` - Trained XGBoost model

---

## Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'cv2'`
```bash
pip install opencv-python opencv-contrib-python
```

### Issue: SIFT features not working
```bash
pip install --upgrade opencv-contrib-python
```

### Issue: Out of memory during feature extraction
- Reduce batch size in `extract_all_features()`
- Use subset of data for initial testing
- Consider downsampling images

### Issue: Dataset path not recognized
- Verify path: `ls /Users/vybhavreddy/Desktop/LISATS`
- Check subdirectories exist: `ls /Users/vybhavreddy/Desktop/LISATS/Stop`
- Update `--data_path` argument if using different location

---

## Performance Tips

1. **Faster Iteration**: Use subset of data first
   ```python
   X = X[:500]  # Test with 500 images
   y = y[:500]
   ```

2. **Parallel Processing**: Already enabled in:
   - Random Forest: `n_jobs=-1`
   - k-NN: `n_jobs=-1`
   - Model evaluation: `cross_validate` with `n_jobs=1` (to avoid nested parallelism)

3. **GPU Acceleration**: XGBoost supports GPU if available
   - Uncomment in `models.py`: `tree_method='gpu_hist'`

4. **Caching Features**: Save extracted features to avoid re-extraction
   ```python
   np.save('X_train_features.npy', X_train_features)
   ```

---

## Next Steps

1. **Run the pipeline**: Execute `python train.py`
2. **Analyze results**: Open `notebooks/EDA.ipynb` in Jupyter
3. **Generate report**: Write 6-8 page report using `REPORT_TEMPLATE.md`
4. **Create slides**: Prepare 10-12 slide presentation using `PRESENTATION_OUTLINE.md`
5. **Deploy model**: Package best model for production use

---

## References

- [scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [OpenCV Documentation](https://docs.opencv.org/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)

---

## Project Timeline

- âœ… **Week 1**: Exploratory Data Analysis
- âœ… **Week 2**: Feature Engineering & Implementation
- âœ… **Week 3**: Model Training & CV
- ðŸ“‹ **Week 4**: Evaluation & Robustness Analysis
- ðŸ“‹ **Week 5**: Report Writing & Presentation

---

**Last Updated**: December 2024  
**Maintainer**: [Your Name]
